Cloud computing is dramatically changing the management of computing
infrastructures, as explained by~\cite{WPonMckinsey13}. On one hand,
cloud providers such as Amazon EC2 allow companies to deploy their
computing products on large infrastructures with a limited start up
cost, as discussed by~\cite{Buyya09:FGCS}. On the other hand, the
flexibility offered by the cloud infrastructure itself is favoring the
adoption of private clouds~\cite{Gulati11:HotCloud} therefore the companies are converting their own computing infrastructures into small internally-managed clouds.

One of the main advantages offered by cloud infrastructure is their
{\bf elasticity}. If a company develops a service and deploy it on the
cloud, the company can rapidly change the characteristics of the
portion of the cloud infrastructure that is offering the service,
automatically acquiring and releasing computing
resources~\cite{Herbst13:ICAC}. This is true for public cloud and
partially also for private ones, where the resources given to the
\acp{vm} running the deployed service can be changed dynamically.
Elasticity is often partitioned into two complementary types: vertical
and horizontal. Vertical elasticity consists in adding or removing
resources from an existing \ac{vm} while horizontal elasticity deals
with changing the number of \acp{vm} offering a specific service,
adding a new machine or removing an existing one.

Horizontal elasticity calls for the introduction of a specialized
component, called {\bf load-balancer}, that takes care of routing the
requests to the different \acp{vm} composing the
service. Load-balancing techniques have been widely studied and
adopted. \textcolor{red}{Martina: this last sentence IMHO screams for
  a citation at the end, do we have a good survey to cite?}

One of the main issues with cloud computing infrastructures, however,
is {\bf robustness} to unexpected events. For example, flash-crowds
are sudden increases of end-users, that may increase the required
capacity by up to five times~\cite{Bodik10:SoCC}. Similarly, hardware
failures may temporarily reduce the capacity of the data-center, while
the failure is repaired. Due to the large magnitude and short duration
of such events, it may be economically too costly to provision enough
hardware to properly deal with them. As a results, unexpected events
may lead to data-center overload, which translates to unresponsive
cloud applications, leading to dissatisfied end-users and revenue
loss.

In our previous work~\citep{cloudish-tr}, we proposed a cloud
application development paradigm called {\bf brownout}. The main
characteristic of cloud brownout is to be able to cope with unexpected
events by reducing the amount of computation done in the \ac{vm} to
produce the response. In brownout applications some computations are
marked as optional --- for example the execution of a recommending
system --- and some others are mandatory --- the display of the
product information in an e-commerce website. Whenever a request is
received, the application can choose to execute or not the optional
code based on the data-center conditions and on its own
optimization. Note that executing optional code directly translates
into a better service and more revenues for the company. This approach
proved to be successful for dealing with unexpected events. However,
we only considered services having a single replica and running inside
a single \ac{vm}.

In this paper, we extend our previous work to multiple \acp{vm},
developing a brownout-compliant load-balancer. This enables horizontal
elasticity and makes applications fault-tolerant by having several
replica. Existing, state-of-the-art load-balancers direct load based
on the response-time of each replica, which leads to inefficient
decisions for brownout-compliant applications, since such applications
already keep their response-time at a given setpoint (at the expense
of reducing the amount of optional content served). The reason why a
brownout-compliant load balancer is necessary is that it is not
possible to discriminate between an application that is helping the
infrastructure not executing the optional code, therefore achieving
low response times and an application that is achieving low response
times serving however many requests with optional code
enabled. Therefore, existing load-balancing techniques monitor
response times and therefore decide how to route requests should be
improved to consider the application cooperativeness.

Our challenge is to find a load-balancing methodology that maximizes
the amount of served optional content. Our contribution is threefold.
\begin{enumerate}
\item We formalize our problem (Section~\ref{sec:problem}) and propose
  a load-balancing strategy for brownout-compliant cloud applications.
\item Our load-balancer is based on optimization and maximizes the
  performance of brownout-compliant applications in terms of
  percentage of optional code executed (Section~\ref{sec:solution}).
\item We evaluate the approach showing that it enhances the
  performance of the cloud applications
  (Section~\ref{sec:evaluation}).
\end{enumerate}

