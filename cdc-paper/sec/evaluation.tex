In this section we describe our experimental evaluation, discussing
the performance indicators used to compare different strategies, the
simulator developed and used to emulate the behavior of
brownout-compliant replicas driven by the load-balancer and our case
studies.

\subsection{Performance indicators}

Performance measures are necessary to objectively compare different
algorithms. While our first performance indicator is clearly defined
as the \textbf{percentage $\%_{on}$} of the total requests served with
the optional content enabled, we also would like to introduce some
other performance metrics to compare the implemented load-balancing
techniques.

Another metric considered for this study is the \textbf{user-perceived
  stability $\sigma_u$}~\cite{GeograficalSASO}. This metric refers to
the variation of perfomance as observed by the users, and it is
measured as the standard deviation of the vector of response
times. Its purpose is to measure the ability of the replicas to
respond timely to the client requests. The entire brownout framework
aims at stabilizing the response times, therefore it should achieve
low user-perceived stability, irregardless of the presence of the
load-balancer. However, the load-balancing algorithm clearly
influences the perceived latencies, therefore it is logical to check
whether the newly developed algorithms achieve a better perceived
stability with respect to the classical ones. Together with the value
of the user-perceived stability, we also report the \textbf{average
  response time $\mu_u$} to distinguish between algorithms that
achieve a low response time with possibly high fluctuations from
solutions that achieve a higher but more stable response time.

\subsection{Simulator}

To test our load-balancing strategies, together with existing state of
the art solutions, we implemented a simulator for brownout-compliant
applications. In the simulator, it is very easy to plug-in new
load-balancing algorithms. The simulator is based on the concepts of
\emph{Client}, \emph{Request}, \emph{LoadBalancer}, \emph{Replica} and
\emph{Link}.

When a new client is defined, it can behave according to the open-loop
client model, where it simply issues a certain number of unrelated
requests (as it is true for clients that respect the Markovian
assumption), or according to che closed-loop
one~\cite{openvsclosed}. Closed-loop clients issue a request and wait
for the response, whenever they receive the response they think for
some time and subsequently continue using the application with another
request. While this second model is more realistic, the first one is
still useful to simulate the behavior of the mass. The simulator
implements both of them, to allow for complete test.

Requests are received by the load-balancer, that directs them towards
different replicas. The load-balancer can work on a per-request basis
or based on weights. The first case is used to simulate policies like
Round Robin, Random, Shortest Queue First and so on, that do not rely
on the concept of weights. The weighted load-balancer is used to
simulate the strategies proposed in this paper, as well as state of
the art solutions like the Weighted Round Robin.

Each replica simulates the computation necessary to serve the request
and chooses if it should be executed with or without the optional
components activated. Service times depend on that. The replicas are
also executing an internal control loop to select their control
variables, the probability of executing the optional
components. Details on how the interal control loops are realized can
be found on~\cite{cloudish-tr}. The replicas use Processor Sharing to
process the requests in the queue, meaning that each of the $n$ active
request will get $\frac{1}{n}$-th of the processing capability of the
replica.

Finally, there are network links connecting the load-balancer and the
replicas. These link are part of the simulator, to study the effect of
network delays in communicating the necessary feedback information for
the load-balancer execution.

The simulator receives as input a \emph{Scenario}, which describes
what can happen during the simulation. The scenario definition
supports the insertion of new clients and the removal of existing
ones. It also allows to turn on and off replicas at specific times
during the execution and to change the service times for every
replica, both for the optional components and for the mandatory
ones. This simulates a change in the amount of resources given to the
machine hosting the replica and it is based on the assumption that
these changes are unpredictable and can happen at the architecture
level, for example due to the cloud provider co-locating more
applications onto the same physical hardware, therefore reducing their
computation capability.

With the scenarios, it is very easy to simulate different working
conditions and to have a complete overview of the changes that might
happen during the load-balancing and replica execution. In the
following, we describe two experiments conducted to compare the
load-balancing strategies when subject to different execution
conditions.

\subsection{Reacting to clients behavior}

The aim of this first test is to evaluate the performance of different
algorithms when new clients are arriving and some other clients are
disconnecting.

We present an experiment where the infrastructure is composed by four
replicas. The first replica is the fastest one and requires $0.05$s to
serve a request with the optional content, while only $0.005$s are
necessary to produce the answer without optional components. The
second replica is slower and takes $0.25$s to produce a full request
and $0.025$s to produce the mandatory content. The third and fourth
replicas are the slowest ones, taking $0.5$s for the full content and
$0.05$s for the mandatory content only.

Clients adhere to the closed-loop model. $50$ clients are accessing
the system at the beginning of the simulation, time $0$s, and $10$ of
them are removed after $200$s. At time $400$s, $25$ more clients query
the application and $25$ more arrives again at $600$s. $40$ clients
disconnect at time $800$s and the simulation is ended at time $1000$s.

\begin{figure}
  \centering \input{img/clientchanges-full}
  \caption{Results of a simulation with four replicas and clients
    entering and leaving the system at different time instants. The
    left column shows the effective weights computed by the
    load-balancing policies while the right column shows the control
    variables for each replica. The first replica is depicted in black
    solid lines, the second replica is shown in blue dashed lines, the
    third replica in green dash-dotted lines, and the fourth one in
    red dotted lines.}
\label{fig:clientchanges-full}
\end{figure}

\begin{figure}
\centering
\input{img/clientchanges-boxplot}
\vspace{-4mm}
\caption{Box plot of the maximum response time in all the replicas for
  every control interval. Each box depicts from the first quartile to
  the third. The red line shows the median; outliers are represented
  with red crosses while the black dots indicate the average value
  (also considering the outliers).}
\label{fig:clientchanges-boxplot}
\end{figure}

The right column in Figure~\ref{fig:clientchanges-full} shows the
control variable $\theta_i$ for each replica, while the left column
shows the effective weights $w_i$ that have been assigned by the
load-balancing strategies. Since solutions like the Round Robin do not
assign directly the weights, we decided to compute the effective
values that can be found after the load-balancing assignments. The
algorithms are ordered based on the percentage $\%_{on}$ of optional
content served, where the Equality Principle-Based Heuristic (EPBH)
achieves the best percentage overall, followed by the Variational
Principle-Based Heuristic (VPBH) and by the Optimization Based
Load-Balancer (OBLB).

For this scenario, the strategies that are aware of the adaptation at
the replica level achieve better results in terms of percentage of
optional content served. The Shortest Queue First (SQF) algorithm is
the only existing one capable to achieve similar (yet lower)
performance in terms of optional content delivered.

To analyze the effect of the load-balancing strategies on the replicas
response times, Figure~\ref{fig:clientchanges-boxplot} shows a box
plot of the maximum latency experienced in intervals of time by the
replicas. The load-balancing strategies are ordered from left to right
based on the percentage of optional code $\%_{on}$ achieved. As in
every box plot, the bottom line for each box represents the first
quartile, the top line the third and the red line is the median. The
red crosses depicts the outliers. In addition to the classical box
plot information, the black dots depict for each algorithm the average
value of the maximum latency measured during the experiment, also
considering the outliers.

The box plot clearly shows that all the solution presented in this
paper achieve distributions that have outliers, as well as almost all
the literature ones. The only exception seems to be SQF, that achieves
predictable maximum latency, with a median that is just slightly
higher than the one achieved by VPBH. OBLB offers in this case a
compromise between optional content and the presence of outliers, that
are limited with respect to all the other solutions. EPBH offers the
highest percentage of optional content served, by sacrificing the
latency bound. From this additional information one can conclude that
the solution presented in this paper should be used carefully in
situations where the requirement on the response time is not soft, but
however tend to increase the profit of the application developer (up
to $15\%$ more optional code served with respect to the best solution
for the case study). Notice also that the proposed heuristics (EPBH
and VPBH) have tunable parameters that can be used to exploit the
trade-off between latency bounds and optional content served.

Notice that this case study is somehow small, featuring only four
replicas.  However, we have conducted additional tests, also in more
complex scenarios, reporting results similar to the ones presented
herein. In the next section we test the effect of infrastructural
changes to load-balancing solutions and response times.

\subsection{Reacting to infrastructure resources}

In this second case study the architecture is composed by five
replicas, that are assigned resources. At time $0$s, the first replica
can serve requests with the optional components consuming $0.07$s per
request, while generating only the mandatory content takes
$0.001$s. The second and third replicas are slower and uses $0.14$s
and $0.002$s for full and mandatory content. The fourth replica is the
slowest requiring $0.7$s and $0.01$s.

At time $250$s we simulate a decrease in the amount of resources
assigned to the first replica, that can now serve optional components
taking $0.35$s and only the mandatory part of the response consuming
$0.005$s. At time $500$s, the fourth replica receives more resources
serving the optional content for $0.07$s and the mandatory only for
$0.001$s. The same happens at time $750$ to the third replica.

\begin{table}
\centering
\caption{Performance with variable infrastructure resources}
\label{tab:resourcechanges-performance}
\begin{tabular}{l c c c}
\hline
Algorithm   & $\%_{on}$ & $\mu_u$ & $\sigma_u$ \\
\hline
OBLB        & $\mathbf{91.4\%}$ & $0.71$          & $0.89$ \\
EPBH        & $89.5\%$          & $1.06$          & $1.95$ \\
VPBH        & $87.7\%$          & $1.02$          & $1.90$ \\
SQF         & $83.3\%$          & $\mathbf{0.55}$ & $\mathbf{0.40}$ \\
RR          & $75,5\%$          & $1.11$          & $2.42$ \\
Random      & $72.9\%$          & $0.86$          & $2.23$ \\
2RC         & $72.2\%$          & $0.74$          & $1.64$ \\
FRF         & $70.4\%$          & $1.27$          & $2.03$ \\
Weighted-RR & $60.7\%$          & $0.90$          & $2.97$ \\
FRF-EWMA    & $51.4\%$          & $1.44$          & $3.41$ \\
Predictive  & $47.4\%$          & $1.66$          & $3.48$ \\
\hline
\end{tabular}
\end{table}

Table~\ref{tab:resourcechanges-performance} reports the percentage
$\%_{on}$, the average response time and the user-perceived stability
for the different algorithms. It should be noted again that the
developed strategies obtain better optional content served, however
slightly penalizing response times. However, the Optimization Based
Load-Balancer is capable of obtaining both low latency and high
percentage of optional content served. This is due to the amount of
information that it uses, since we assume that the computation times
for mandatory and optional part are known. The optimization-based
strategy is capable of reacting fast to changes and achieves
predictability in the application behavior. Again, if one does not
have all the necessary information available, it is possible to
implement strategies that would better exploit the trade-off between
bounded latency and optional content.
